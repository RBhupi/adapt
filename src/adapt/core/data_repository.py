"""Centralized data access and artifact management for ADAPT pipeline.

Provides a unified API for:
- Registering artifacts with lineage tracking
- Writing NetCDF, Parquet, and SQLite data atomically
- Querying artifacts by type, time range, and radar
- Opening datasets and tables for analysis

The DataRepository is the single source of truth for all pipeline artifacts.
No other component should directly access the filesystem for data I/O.
"""

import hashlib
import json
import logging
import os
import shutil
import sqlite3
import tempfile
import threading
import uuid
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, TYPE_CHECKING

import pandas as pd
import xarray as xr

if TYPE_CHECKING:
    from adapt.schemas import InternalConfig

__all__ = ['DataRepository', 'ProductType']

logger = logging.getLogger(__name__)


class ProductType:
    """Artifact product type constants."""
    NEXRAD_RAW = "nexrad_raw"
    GRIDDED_NC = "gridded_nc"
    ANALYSIS_NC = "analysis_nc"
    CELLS_DB = "cells_db"
    CELLS_PARQUET = "cells_parquet"
    PLOT_PNG = "plot_png"


class DataRepository:
    """Centralized artifact management for radar processing pipeline.

    Manages the complete lifecycle of pipeline artifacts:
    - Registration in per-run SQLite catalog
    - Atomic file writes with lineage tracking
    - Processing status management
    - Query API for downstream consumers

    Directory Structure:
        root_dir/
        ├── catalog/{run_id}_catalog.sqlite
        ├── RADAR_ID/
        │   ├── nexrad/YYYYMMDD/{original_aws_name}
        │   ├── gridnc/YYYYMMDD/{file_stem}_{run_id}_gridded.nc
        │   ├── analysis/YYYYMMDD/{file_stem}_{run_id}_analysis.nc
        │   ├── analysis/YYYYMMDD/{radar_id}_{run_id}_cells.db
        │   └── plots/YYYYMMDD/{radar_id}_{plot_type}_{HHMMSS}_{run_id}.png
        └── logs/

    Thread Safety:
        All methods are thread-safe via internal locking.
        Multiple threads can register and query artifacts concurrently.

    Example usage::

        # Create repository for a pipeline run
        repo = DataRepository(
            run_id="abc12345",
            base_dir="/data/radar_output",
            radar_id="KDIX"
        )

        # Write gridded NetCDF with lineage
        gridded_id = repo.write_netcdf(
            ds=gridded_dataset,
            product_type=ProductType.GRIDDED_NC,
            scan_time=datetime(2026, 2, 11, 12, 0, 0),
            producer="loader",
            parent_ids=[raw_artifact_id],
            metadata={"grid_shape": [41, 201, 201]}
        )

        # Query artifacts
        grids = repo.query(product_type=ProductType.GRIDDED_NC)
    """

    def __init__(
        self,
        run_id: str,
        base_dir: Union[str, Path],
        radar_id: str,
        config: Optional["InternalConfig"] = None,
    ):
        """Initialize repository for a pipeline run.

        Parameters
        ----------
        run_id : str
            Unique 8-character identifier for this pipeline run.
            Generated by orchestrator at startup.
        base_dir : str or Path
            Root output directory for all artifacts.
        radar_id : str
            Radar station identifier (e.g., "KDIX").
        config : InternalConfig, optional
            Validated runtime configuration.
        """
        self.run_id = run_id
        self.base_dir = Path(base_dir).resolve()
        self.radar_id = radar_id
        self.config = config

        # Create directory structure
        self._init_directories()

        # Catalog database path
        self.catalog_path = self.catalog_dir / f"{run_id}_catalog.sqlite"

        # Thread safety
        self._lock = threading.RLock()
        self._conn: Optional[sqlite3.Connection] = None

        # Initialize database
        self._init_catalog()

        logger.info(f"DataRepository initialized: run_id={run_id}, radar_id={radar_id}, catalog={self.catalog_path}")

    def _init_directories(self) -> None:
        """Create directory structure for this radar."""
        # Catalog directory at root level
        self.catalog_dir = self.base_dir / "catalog"
        self.catalog_dir.mkdir(parents=True, exist_ok=True)

        # Radar-specific directories: root_dir/RADAR_ID/type/
        radar_base = self.base_dir / self.radar_id

        self.output_dirs = {
            "base": self.base_dir,
            "catalog": self.catalog_dir,
            "nexrad": radar_base / "nexrad",
            "gridnc": radar_base / "gridnc",
            "analysis": radar_base / "analysis",
            "plots": radar_base / "plots",
            "logs": self.base_dir / "logs",
        }

        # Create all directories
        for path in self.output_dirs.values():
            path.mkdir(parents=True, exist_ok=True)

    # =========================================================================
    # Database Initialization
    # =========================================================================

    def _get_connection(self) -> sqlite3.Connection:
        """Get thread-safe database connection."""
        if self._conn is None:
            self._conn = sqlite3.connect(
                str(self.catalog_path),
                check_same_thread=False,
                isolation_level='DEFERRED'
            )
            self._conn.row_factory = sqlite3.Row
            # Enable WAL mode for concurrent reads
            self._conn.execute("PRAGMA journal_mode=WAL")
        return self._conn

    def _init_catalog(self) -> None:
        """Create catalog database schema."""
        conn = self._get_connection()

        with self._lock:
            # Main artifacts table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS artifacts (
                    artifact_id TEXT PRIMARY KEY,
                    run_id TEXT NOT NULL,
                    product_type TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    producer TEXT NOT NULL,
                    parent_ids TEXT,
                    radar_id TEXT NOT NULL,
                    scan_time TEXT,
                    metadata TEXT,
                    status TEXT DEFAULT 'active',
                    created_at TEXT NOT NULL,
                    file_hash TEXT
                )
            """)

            # Processing status table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS processing_status (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    artifact_id TEXT NOT NULL,
                    plugin_name TEXT NOT NULL,
                    status TEXT NOT NULL,
                    updated_at TEXT NOT NULL,
                    error_message TEXT,
                    output_artifact_id TEXT,
                    FOREIGN KEY (artifact_id) REFERENCES artifacts(artifact_id),
                    UNIQUE(artifact_id, plugin_name)
                )
            """)

            # Runs table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS runs (
                    run_id TEXT PRIMARY KEY,
                    start_time TEXT NOT NULL,
                    end_time TEXT,
                    mode TEXT,
                    config_hash TEXT,
                    radar_id TEXT NOT NULL,
                    status TEXT DEFAULT 'running'
                )
            """)

            # Create indexes
            conn.execute("CREATE INDEX IF NOT EXISTS idx_artifacts_product_type ON artifacts(product_type)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_artifacts_radar_id ON artifacts(radar_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_artifacts_scan_time ON artifacts(scan_time)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_processing_status_artifact ON processing_status(artifact_id)")

            conn.commit()

            # Register this run
            self._register_run()

    def _register_run(self) -> None:
        """Register current pipeline run in catalog."""
        conn = self._get_connection()
        config_hash = self._compute_config_hash() if self.config else None
        mode = self.config.mode if self.config else None

        conn.execute("""
            INSERT OR IGNORE INTO runs (run_id, start_time, mode, config_hash, radar_id, status)
            VALUES (?, ?, ?, ?, ?, 'running')
        """, (
            self.run_id,
            datetime.now(timezone.utc).isoformat(),
            mode,
            config_hash,
            self.radar_id
        ))
        conn.commit()

    def _compute_config_hash(self) -> str:
        """Compute hash of configuration for reproducibility tracking."""
        if self.config is None:
            return ""
        config_str = json.dumps(self.config.model_dump(), sort_keys=True, default=str)
        return hashlib.sha256(config_str.encode()).hexdigest()[:16]

    # =========================================================================
    # Artifact ID Generation
    # =========================================================================

    @staticmethod
    def generate_artifact_id(
        product_type: str,
        radar_id: str,
        scan_time: Optional[datetime],
        run_id: str,
        content_hint: str = ""
    ) -> str:
        """Generate deterministic artifact ID from content metadata.

        Parameters
        ----------
        product_type : str
            Type of artifact (e.g., ProductType.GRIDDED_NC)
        radar_id : str
            Radar station identifier
        scan_time : datetime or None
            Timestamp of radar scan
        run_id : str
            Pipeline run identifier
        content_hint : str, optional
            Additional content for uniqueness (e.g., file hash)

        Returns
        -------
        str
            16-character hash-based artifact ID
        """
        time_str = scan_time.isoformat() if scan_time else "none"
        content = f"{product_type}:{radar_id}:{time_str}:{run_id}:{content_hint}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    # =========================================================================
    # Public API: Registration
    # =========================================================================

    def register_artifact(
        self,
        product_type: str,
        file_path: Union[str, Path],
        scan_time: Optional[datetime] = None,
        producer: str = "unknown",
        parent_ids: Optional[List[str]] = None,
        metadata: Optional[Dict] = None
    ) -> str:
        """Register an artifact in the catalog.

        Parameters
        ----------
        product_type : str
            Artifact type (use ProductType constants)
        file_path : str or Path
            Absolute path to artifact file
        scan_time : datetime, optional
            Timestamp of radar scan
        producer : str
            Name of component that created this artifact
        parent_ids : list of str, optional
            Artifact IDs of parent artifacts (for lineage)
        metadata : dict, optional
            Additional metadata (stored as JSON)

        Returns
        -------
        str
            Unique artifact ID
        """
        file_path = Path(file_path).resolve()
        parent_ids = parent_ids or []
        metadata = metadata or {}

        # Generate artifact ID
        artifact_id = self.generate_artifact_id(
            product_type=product_type,
            radar_id=self.radar_id,
            scan_time=scan_time,
            run_id=self.run_id,
            content_hint=str(file_path)
        )

        conn = self._get_connection()

        with self._lock:
            conn.execute("""
                INSERT OR REPLACE INTO artifacts
                (artifact_id, run_id, product_type, file_path, producer,
                 parent_ids, radar_id, scan_time, metadata, status, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'active', ?)
            """, (
                artifact_id,
                self.run_id,
                product_type,
                str(file_path),
                producer,
                json.dumps(parent_ids),
                self.radar_id,
                scan_time.isoformat() if scan_time else None,
                json.dumps(metadata),
                datetime.now(timezone.utc).isoformat()
            ))
            conn.commit()

        logger.debug(f"Registered artifact: {artifact_id} ({product_type})")
        return artifact_id

    # =========================================================================
    # Public API: Processing Status
    # =========================================================================

    def get_unprocessed(
        self,
        product_type: str,
        plugin_name: str,
        limit: Optional[int] = None
    ) -> List[Dict]:
        """Get artifacts pending processing by a plugin.

        Parameters
        ----------
        product_type : str
            Type of artifacts to query
        plugin_name : str
            Name of processing plugin
        limit : int, optional
            Maximum number of results

        Returns
        -------
        list of dict
            Artifacts that have not been processed by the plugin
        """
        conn = self._get_connection()

        query = """
            SELECT a.* FROM artifacts a
            LEFT JOIN processing_status ps
                ON a.artifact_id = ps.artifact_id AND ps.plugin_name = ?
            WHERE a.product_type = ?
                AND a.run_id = ?
                AND (ps.status IS NULL OR ps.status = 'pending')
            ORDER BY a.scan_time
        """
        params: List = [plugin_name, product_type, self.run_id]

        if limit:
            query += " LIMIT ?"
            params.append(limit)

        with self._lock:
            cursor = conn.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]

    def mark_complete(
        self,
        artifact_id: str,
        plugin_name: str,
        status: str = "completed",
        error_message: Optional[str] = None,
        output_artifact_id: Optional[str] = None
    ) -> None:
        """Mark artifact as processed by a plugin.

        Parameters
        ----------
        artifact_id : str
            ID of processed artifact
        plugin_name : str
            Name of processing plugin
        status : str
            Processing status ('completed', 'failed', 'skipped')
        error_message : str, optional
            Error details if status is 'failed'
        output_artifact_id : str, optional
            ID of artifact produced by this processing step
        """
        conn = self._get_connection()

        with self._lock:
            conn.execute("""
                INSERT OR REPLACE INTO processing_status
                (artifact_id, plugin_name, status, updated_at, error_message, output_artifact_id)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                artifact_id,
                plugin_name,
                status,
                datetime.now(timezone.utc).isoformat(),
                error_message,
                output_artifact_id
            ))
            conn.commit()

        logger.debug(f"Marked {artifact_id} as {status} by {plugin_name}")

    # =========================================================================
    # Public API: Data Access
    # =========================================================================

    def open_dataset(self, artifact_id: str) -> xr.Dataset:
        """Open NetCDF artifact as xarray Dataset.

        Parameters
        ----------
        artifact_id : str
            Artifact ID (must be NetCDF type)

        Returns
        -------
        xr.Dataset
            Opened dataset (caller responsible for closing)

        Raises
        ------
        ValueError
            If artifact not found or not a NetCDF type
        FileNotFoundError
            If file does not exist
        """
        artifact = self._get_artifact(artifact_id)

        if artifact is None:
            raise ValueError(f"Artifact not found: {artifact_id}")

        product_type = artifact['product_type']
        if product_type not in (ProductType.GRIDDED_NC, ProductType.ANALYSIS_NC):
            raise ValueError(f"Cannot open as dataset: {product_type}")

        file_path = Path(artifact['file_path'])
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        return xr.open_dataset(file_path)

    def open_table(
        self,
        artifact_id: str,
        table_name: Optional[str] = None
    ) -> pd.DataFrame:
        """Open SQLite or Parquet artifact as DataFrame.

        Parameters
        ----------
        artifact_id : str
            Artifact ID (must be CELLS_DB or CELLS_PARQUET type)
        table_name : str, optional
            Table name for SQLite (default: 'cells')

        Returns
        -------
        pd.DataFrame
            Data from artifact

        Raises
        ------
        ValueError
            If artifact not found or invalid type
        """
        artifact = self._get_artifact(artifact_id)

        if artifact is None:
            raise ValueError(f"Artifact not found: {artifact_id}")

        product_type = artifact['product_type']
        file_path = Path(artifact['file_path'])

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        if product_type == ProductType.CELLS_PARQUET:
            return pd.read_parquet(file_path)
        elif product_type == ProductType.CELLS_DB:
            table_name = table_name or 'cells'
            with sqlite3.connect(str(file_path)) as conn:
                return pd.read_sql(f"SELECT * FROM {table_name}", conn)
        else:
            raise ValueError(f"Cannot open as table: {product_type}")

    def query(
        self,
        product_type: Optional[str] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
        radar_id: Optional[str] = None,
        limit: Optional[int] = None
    ) -> List[Dict]:
        """Query artifacts by criteria.

        Parameters
        ----------
        product_type : str, optional
            Filter by product type
        time_range : tuple of datetime, optional
            (start_time, end_time) filter
        radar_id : str, optional
            Filter by radar ID (defaults to repository radar_id)
        limit : int, optional
            Maximum results

        Returns
        -------
        list of dict
            Matching artifacts
        """
        conn = self._get_connection()

        conditions = ["run_id = ?"]
        params: List = [self.run_id]

        if product_type:
            conditions.append("product_type = ?")
            params.append(product_type)

        if time_range:
            conditions.append("scan_time >= ? AND scan_time <= ?")
            params.extend([time_range[0].isoformat(), time_range[1].isoformat()])

        if radar_id:
            conditions.append("radar_id = ?")
            params.append(radar_id)

        where_clause = " AND ".join(conditions)
        query_str = f"SELECT * FROM artifacts WHERE {where_clause} ORDER BY scan_time"

        if limit:
            query_str += f" LIMIT {limit}"

        with self._lock:
            cursor = conn.execute(query_str, params)
            return [dict(row) for row in cursor.fetchall()]

    def get_latest(
        self,
        product_type: str,
        radar_id: Optional[str] = None
    ) -> Optional[Dict]:
        """Get the most recent artifact of a given type.

        Queries the catalog for the latest artifact by scan_time descending.
        Used by downstream consumers (e.g., plot consumer) to poll for new data.

        Parameters
        ----------
        product_type : str
            Artifact type to query (use ProductType constants)
        radar_id : str, optional
            Filter by radar ID (defaults to repository radar_id)

        Returns
        -------
        dict or None
            Most recent artifact record, or None if no artifacts exist
        """
        conn = self._get_connection()

        conditions = ["run_id = ?", "product_type = ?"]
        params: List = [self.run_id, product_type]

        if radar_id:
            conditions.append("radar_id = ?")
            params.append(radar_id)

        where_clause = " AND ".join(conditions)
        query_str = f"""
            SELECT * FROM artifacts
            WHERE {where_clause}
            ORDER BY scan_time DESC
            LIMIT 1
        """

        with self._lock:
            cursor = conn.execute(query_str, params)
            row = cursor.fetchone()
            return dict(row) if row else None

    def get_all_since(
        self,
        product_type: str,
        since_artifact_id: Optional[str] = None,
        radar_id: Optional[str] = None
    ) -> List[Dict]:
        """Get all artifacts of a type created after a given artifact.

        Used by consumers to catch up on missed artifacts during polling gaps.

        Parameters
        ----------
        product_type : str
            Artifact type to query
        since_artifact_id : str, optional
            Return artifacts created after this artifact's scan_time.
            If None, returns all artifacts of this type.
        radar_id : str, optional
            Filter by radar ID

        Returns
        -------
        list of dict
            Artifacts in chronological order (oldest first)
        """
        conn = self._get_connection()

        conditions = ["run_id = ?", "product_type = ?"]
        params: List = [self.run_id, product_type]

        if radar_id:
            conditions.append("radar_id = ?")
            params.append(radar_id)

        # If since_artifact_id provided, get its scan_time and filter
        if since_artifact_id:
            ref_artifact = self._get_artifact(since_artifact_id)
            if ref_artifact and ref_artifact.get('scan_time'):
                conditions.append("scan_time > ?")
                params.append(ref_artifact['scan_time'])

        where_clause = " AND ".join(conditions)
        query_str = f"""
            SELECT * FROM artifacts
            WHERE {where_clause}
            ORDER BY scan_time ASC
        """

        with self._lock:
            cursor = conn.execute(query_str, params)
            return [dict(row) for row in cursor.fetchall()]

    def get_artifact(self, artifact_id: str) -> Optional[Dict]:
        """Get artifact record by ID (public method).

        Parameters
        ----------
        artifact_id : str
            Artifact ID to retrieve

        Returns
        -------
        dict or None
            Artifact record or None if not found
        """
        return self._get_artifact(artifact_id)

    # =========================================================================
    # Public API: Write Operations
    # =========================================================================

    def write_netcdf(
        self,
        ds: xr.Dataset,
        product_type: str,
        scan_time: datetime,
        producer: str,
        parent_ids: Optional[List[str]] = None,
        metadata: Optional[Dict] = None,
        filename_stem: Optional[str] = None
    ) -> str:
        """Write xarray Dataset to NetCDF and register artifact.

        Uses atomic write (temp file + rename) for safety.

        Parameters
        ----------
        ds : xr.Dataset
            Dataset to write
        product_type : str
            Must be GRIDDED_NC or ANALYSIS_NC
        scan_time : datetime
            Scan timestamp
        producer : str
            Component name
        parent_ids : list of str, optional
            Parent artifact IDs
        metadata : dict, optional
            Additional metadata
        filename_stem : str, optional
            Base filename (without extension). Auto-generated if None.

        Returns
        -------
        str
            Registered artifact ID
        """
        # Generate output path
        output_path = self._generate_netcdf_path(
            product_type=product_type,
            scan_time=scan_time,
            filename_stem=filename_stem
        )

        # Atomic write
        self._atomic_write_netcdf(ds, output_path)

        # Register artifact
        return self.register_artifact(
            product_type=product_type,
            file_path=output_path,
            scan_time=scan_time,
            producer=producer,
            parent_ids=parent_ids,
            metadata=metadata
        )

    def write_parquet(
        self,
        df: pd.DataFrame,
        product_type: str,
        scan_time: datetime,
        producer: str,
        parent_ids: Optional[List[str]] = None,
        metadata: Optional[Dict] = None,
        filename_stem: Optional[str] = None
    ) -> str:
        """Write DataFrame to Parquet and register artifact.

        Parameters
        ----------
        df : pd.DataFrame
            DataFrame to write
        product_type : str
            Must be CELLS_PARQUET
        scan_time : datetime
            Scan timestamp
        producer : str
            Component name
        parent_ids : list of str, optional
            Parent artifact IDs
        metadata : dict, optional
            Additional metadata
        filename_stem : str, optional
            Base filename

        Returns
        -------
        str
            Registered artifact ID
        """
        # Generate output path
        output_path = self._generate_parquet_path(
            scan_time=scan_time,
            filename_stem=filename_stem
        )

        # Atomic write
        self._atomic_write_parquet(df, output_path)

        # Update metadata with row count
        metadata = metadata or {}
        metadata['row_count'] = len(df)

        # Register artifact
        return self.register_artifact(
            product_type=product_type,
            file_path=output_path,
            scan_time=scan_time,
            producer=producer,
            parent_ids=parent_ids,
            metadata=metadata
        )

    def write_sqlite_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        artifact_id: str,
        if_exists: str = 'append'
    ) -> None:
        """Write DataFrame to SQLite table within an existing artifact.

        Performs automatic schema migration when appending: if the DataFrame
        has columns not present in the existing table, those columns are
        added via ALTER TABLE before the insert.

        Parameters
        ----------
        df : pd.DataFrame
            Data to write
        table_name : str
            Table name
        artifact_id : str
            Existing CELLS_DB artifact ID
        if_exists : str
            Pandas to_sql behavior ('append', 'replace', 'fail')
        """
        artifact = self._get_artifact(artifact_id)

        if artifact is None:
            raise ValueError(f"Artifact not found: {artifact_id}")

        file_path = Path(artifact['file_path'])

        with sqlite3.connect(str(file_path)) as conn:
            # If appending, perform schema migration for missing columns
            if if_exists == 'append':
                self._migrate_table_schema(conn, table_name, df)

            df.to_sql(table_name, conn, if_exists=if_exists, index=False)

        logger.debug(f"Wrote {len(df)} rows to {table_name} in {artifact_id}")

    def _migrate_table_schema(
        self,
        conn: sqlite3.Connection,
        table_name: str,
        df: pd.DataFrame
    ) -> None:
        """Add missing columns to existing table before append.

        Uses the same type mapping rules as processor._create_cells_table:
        - *_x, *_y suffix → INTEGER (pixel coordinates)
        - *_lat, *_lon suffix → REAL (geographic coordinates)
        - *_json suffix → TEXT (JSON data)
        - radar_* prefix → REAL (radar measurements)
        - *_mean, *_min, *_max, *_sqkm suffix → REAL (statistics)
        - time* columns → TIMESTAMP
        - cell_label → INTEGER
        - Default → TEXT
        """
        # Check if table exists
        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
            (table_name,)
        )
        if cursor.fetchone() is None:
            # Table doesn't exist yet, will be created by to_sql
            return

        # Get existing columns
        cursor = conn.execute(f"PRAGMA table_info('{table_name}')")
        existing_columns = {row[1] for row in cursor.fetchall()}

        # Find missing columns
        df_columns = set(df.columns)
        missing_columns = df_columns - existing_columns

        if not missing_columns:
            return

        # Add missing columns
        for col in missing_columns:
            sql_type = self._infer_sql_type(col)
            try:
                conn.execute(f'ALTER TABLE "{table_name}" ADD COLUMN "{col}" {sql_type}')
                logger.debug(f"Added column '{col}' ({sql_type}) to table '{table_name}'")
            except sqlite3.OperationalError as e:
                # Column might already exist (race condition)
                if "duplicate column name" not in str(e).lower():
                    raise

        conn.commit()
        logger.info(f"Schema migration: added {len(missing_columns)} columns to '{table_name}'")

    @staticmethod
    def _infer_sql_type(column_name: str) -> str:
        """Infer SQL type from column name using processor schema conventions.

        Type mapping rules (matching processor._create_cells_table):
        - Pixel coordinates (*_x, *_y) → INTEGER
        - Geographic coordinates (*_lat, *_lon) → REAL
        - JSON fields (*_json) → TEXT
        - Radar measurements (radar_*) → REAL
        - Statistics (*_mean, *_min, *_max, *_sqkm) → REAL
        - Timestamps (time*) → TIMESTAMP
        - Cell identifier (cell_label) → INTEGER
        - Default → TEXT
        """
        col = column_name.lower()

        # Integer types
        if col.endswith('_x') or col.endswith('_y'):
            return 'INTEGER'
        if col == 'cell_label':
            return 'INTEGER NOT NULL'

        # Real/float types
        if col.endswith('_lat') or col.endswith('_lon'):
            return 'REAL'
        if col.startswith('radar_'):
            return 'REAL'
        if col.endswith(('_mean', '_min', '_max', '_sqkm')):
            return 'REAL'
        if 'heading' in col:
            return 'REAL'
        if 'area' in col:
            return 'REAL'

        # Timestamp types
        if col.startswith('time'):
            return 'TIMESTAMP'

        # Text types
        if col.endswith('_json'):
            return 'TEXT'

        # Default to TEXT for safety
        return 'TEXT'

    def get_or_create_cells_db(
        self,
        scan_time: datetime,
        producer: str,
        parent_ids: Optional[List[str]] = None
    ) -> str:
        """Get existing cells database or create new one.

        Parameters
        ----------
        scan_time : datetime
            Scan timestamp (used for path generation)
        producer : str
            Component name
        parent_ids : list of str, optional
            Parent artifact IDs

        Returns
        -------
        str
            Artifact ID of cells database
        """
        # Check for existing
        existing = self.query(product_type=ProductType.CELLS_DB)

        if existing:
            return existing[0]['artifact_id']

        # Create new
        output_path = self._generate_sqlite_path(scan_time)

        # Create empty database with WAL mode
        self._init_cells_db(output_path)

        return self.register_artifact(
            product_type=ProductType.CELLS_DB,
            file_path=output_path,
            scan_time=scan_time,
            producer=producer,
            parent_ids=parent_ids or [],
            metadata={}
        )

    # =========================================================================
    # Internal: Path Generation
    # =========================================================================

    def _generate_netcdf_path(
        self,
        product_type: str,
        scan_time: datetime,
        filename_stem: Optional[str] = None
    ) -> Path:
        """Generate NetCDF output path with run_id suffix.

        Pattern: root_dir/RADAR_ID/type/YYYYMMDD/filename_{run_id}_{suffix}.nc
        """
        date_str = scan_time.strftime("%Y%m%d")
        time_str = scan_time.strftime("%H%M%S")

        if filename_stem is None:
            filename_stem = f"{self.radar_id}{date_str}_{time_str}"

        if product_type == ProductType.GRIDDED_NC:
            base_dir = self.output_dirs["gridnc"] / date_str
            suffix = "gridded"
        else:  # ANALYSIS_NC
            base_dir = self.output_dirs["analysis"] / date_str
            suffix = "analysis"

        base_dir.mkdir(parents=True, exist_ok=True)
        filename = f"{filename_stem}_{self.run_id}_{suffix}.nc"

        return base_dir / filename

    def _generate_parquet_path(
        self,
        scan_time: datetime,
        filename_stem: Optional[str] = None
    ) -> Path:
        """Generate Parquet output path with run_id suffix.

        Pattern: root_dir/RADAR_ID/analysis/YYYYMMDD/filename_{run_id}_cells.parquet
        """
        date_str = scan_time.strftime("%Y%m%d")

        base_dir = self.output_dirs["analysis"] / date_str
        base_dir.mkdir(parents=True, exist_ok=True)

        if filename_stem is None:
            filename = f"{self.radar_id}_{self.run_id}_cells.parquet"
        else:
            filename = f"{filename_stem}_{self.run_id}.parquet"

        return base_dir / filename

    def _generate_sqlite_path(self, scan_time: datetime) -> Path:
        """Generate SQLite database path with run_id suffix.

        Pattern: root_dir/RADAR_ID/analysis/YYYYMMDD/{radar_id}_{run_id}_cells.db
        """
        date_str = scan_time.strftime("%Y%m%d")

        base_dir = self.output_dirs["analysis"] / date_str
        base_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{self.radar_id}_{self.run_id}_cells.db"
        return base_dir / filename

    def generate_plot_path(
        self,
        plot_type: str,
        scan_time: datetime
    ) -> Path:
        """Generate plot output path with run_id suffix.

        Pattern: root_dir/RADAR_ID/plots/YYYYMMDD/{radar_id}_{plot_type}_{HHMMSS}_{run_id}.png
        """
        date_str = scan_time.strftime("%Y%m%d")
        time_str = scan_time.strftime("%H%M%S")

        base_dir = self.output_dirs["plots"] / date_str
        base_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{self.radar_id}_{plot_type}_{time_str}_{self.run_id}.png"
        return base_dir / filename

    # =========================================================================
    # Internal: Atomic Writes
    # =========================================================================

    def _atomic_write_netcdf(self, ds: xr.Dataset, output_path: Path) -> None:
        """Write NetCDF atomically (temp file + rename)."""
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Write to temp file
        fd, temp_path = tempfile.mkstemp(suffix='.nc', dir=output_path.parent)
        os.close(fd)

        try:
            encoding = {var: {"zlib": True, "complevel": 4} for var in ds.data_vars}
            ds.to_netcdf(temp_path, encoding=encoding, engine='netcdf4')

            # Atomic rename
            shutil.move(temp_path, output_path)
            logger.debug(f"Wrote NetCDF: {output_path}")

        except Exception:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise

    def _atomic_write_parquet(self, df: pd.DataFrame, output_path: Path) -> None:
        """Write Parquet atomically (temp file + rename)."""
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Write to temp file
        fd, temp_path = tempfile.mkstemp(suffix='.parquet', dir=output_path.parent)
        os.close(fd)

        try:
            compression = 'snappy'
            if self.config and hasattr(self.config, 'output'):
                compression = getattr(self.config.output, 'compression', 'snappy')

            df.to_parquet(temp_path, engine='pyarrow', compression=compression, index=False)

            # Atomic rename
            shutil.move(temp_path, output_path)
            logger.debug(f"Wrote Parquet: {output_path}")

        except Exception:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise

    def _init_cells_db(self, db_path: Path) -> None:
        """Initialize cells database with WAL mode."""
        db_path.parent.mkdir(parents=True, exist_ok=True)

        with sqlite3.connect(str(db_path)) as conn:
            # Enable WAL mode for concurrent access
            conn.execute("PRAGMA journal_mode=WAL")
            conn.commit()

    # =========================================================================
    # Internal: Helpers
    # =========================================================================

    def _get_artifact(self, artifact_id: str) -> Optional[Dict]:
        """Get artifact record by ID."""
        conn = self._get_connection()

        with self._lock:
            cursor = conn.execute(
                "SELECT * FROM artifacts WHERE artifact_id = ?",
                (artifact_id,)
            )
            row = cursor.fetchone()
            return dict(row) if row else None

    # =========================================================================
    # Lifecycle
    # =========================================================================

    def finalize_run(self, status: str = "completed") -> None:
        """Mark pipeline run as complete."""
        conn = self._get_connection()

        with self._lock:
            conn.execute("""
                UPDATE runs SET end_time = ?, status = ? WHERE run_id = ?
            """, (
                datetime.now(timezone.utc).isoformat(),
                status,
                self.run_id
            ))
            conn.commit()

        logger.info(f"Run {self.run_id} finalized with status: {status}")

    def close(self) -> None:
        """Close database connection."""
        if self._conn:
            self._conn.close()
            self._conn = None
            logger.debug("DataRepository connection closed")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    # =========================================================================
    # Utility: Run ID Generation
    # =========================================================================

    @staticmethod
    def generate_run_id() -> str:
        """Generate a unique 8-character run ID.

        Returns
        -------
        str
            8-character hexadecimal UUID prefix
        """
        return uuid.uuid4().hex[:8]
