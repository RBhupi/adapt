"""Radar data processing pipeline.

Processes radar files through segmentation, projection, and analysis stages.
Reads Level-II data or gridded NetCDF, segments cells, projects motion,
computes statistics, and persists results to NetCDF and SQLite.
"""

import logging
from pathlib import Path
from typing import Optional, Dict, List
import threading
import queue
from datetime import datetime


import pandas as pd
import numpy as np
import xarray as xr
import sqlite3


from adapt.config import PARAM_CONFIG, get_grid_kwargs, get_output_path
from adapt.radar.loader import RadarDataLoader
from adapt.radar.cell_segmenter import RadarCellSegmenter
from adapt.radar.cell_analyzer import RadarCellAnalyzer
from adapt.radar.cell_projector import RadarCellProjector
from adapt.radar.radar_utils import compute_all_cell_centroids

__all__ = ['RadarProcessor']

logger = logging.getLogger(__name__)


class RadarProcessor(threading.Thread):
    """Processes radar files through the complete scientific analysis pipeline.

    This worker thread runs in the background, receiving filepaths from the
    downloader queue and performing all scientific analysis: loading, regridding,
    segmentation, motion projection, and cell statistics extraction.

    **Processing Pipeline:**

    For each file, the processor performs (in order):

    1. **Load & Regrid**: Reads NEXRAD Level-II data or cached gridded NetCDF,
       converts to Cartesian grid using Cressman weighting and ARM Py-ART.

    2. **Segmentation**: Identifies storm cells using reflectivity threshold
       and morphological filtering. Outputs cell_labels array.

    3. **Motion Projection** (frame 2+): Computes optical flow between current
       and previous frame, projects cells forward by 1-5 timesteps. First frame
       has no projections (only segmentation).

    4. **Cell Analysis**: Extracts properties from each cell:
       - Geometry: area, centroid (mass-weighted and geometric)
       - Reflectivity: max, mean, 95th percentile
       - Dimensions: length, width, aspect ratio
       Stores in SQLite database for later analysis.

    5. **Persistence**: Writes segmentation + projections to NetCDF
       (analysis/{radar_id}_analysis_*_segmentation.nc) and queues file
       for visualization.

    **Output Files:**

    - **Gridded NetCDF**: gridded/{radar_id}_*_gridded.nc (regridded data)
    - **Analysis NetCDF**: analysis/{radar_id}_*_analysis_segmentation.nc
      (segmentation + projections + flow fields)
    - **SQLite Database**: analysis/{radar_id}_cells_statistics.db
      (cell properties for all files)

    **Database Schema:**

    SQLite table `cells` (one row per detected cell):
    - Temporal: time, time_volume_start, time_volume_end
    - Identification: cell_label, radar_id, file_id
    - Geometry: cell_area_sqkm, centroids (lat/lon and x/y)
    - Reflectivity: max_dbz, mean_dbz, percentile_95
    - Dimensions: length_km, width_km, aspect_ratio
    - Motion: heading_x, heading_y, projection_distances (1-5 steps)

    **Frame History:**

    Maintains a 2-frame rolling window to compute optical flow. This enables
    motion estimation between frame N and frame N+1. Frame 1 has only
    segmentation; projections begin at frame 2.

    **Thread Safety:**

    Uses SQLite WAL mode for concurrent read access. Main thread can query
    results while processor is still running.

    Example usage (typically called by orchestrator)::

        processor = RadarProcessor(
            input_queue=downloader_queue,
            config=config,
            output_queue=plotter_queue
        )
        processor.start()
        ...
        processor.stop()
        df = processor.get_results()
    """

    def __init__(self, input_queue: queue.Queue, config: Dict = None,
                 output_queue: queue.Queue = None,
                 name: str = "RadarProcessor"):
        """Initialize processor.

        Parameters
        ----------
        input_queue : queue.Queue
            Queue of filepaths from downloader thread. Processor pops filepaths
            and processes them. None signals shutdown.

        config : dict, optional
            Complete pipeline configuration. Required keys:
            
            - `global`: Variable/coordinate name mappings and z-level
            - `regridder`: Grid parameters (shape, limits, weighting function)
            - `segmenter`: Threshold, morphological settings, size filters
            - `analyzer`: Which cell properties to compute
            - `projector`: Motion estimation and projection steps
            - `output_dirs`: Paths to save gridded, analysis, and logs
            - `downloader`: Radar ID for file naming
            
            If None, uses PARAM_CONFIG (expert defaults).

        output_queue : queue.Queue, optional
            Queue to send analysis files to plotter thread. One dict per file:
            
            - `segmentation_nc`: Path to analysis NetCDF file
            - `radar_id`: Radar identifier
            - `timestamp`: Datetime of the scan
            
            If None, no plotting occurs (analysis-only mode).

        name : str, optional
            Thread name for logging (default: "RadarProcessor").

        Raises
        ------
        Exception
            If database initialization fails or output directories don't exist.
        """
        super().__init__(daemon=True, name=name)

        self.input_queue = input_queue
        self.config = config or PARAM_CONFIG
        self.output_queue = output_queue  # For plotter thread
        self._stop_event = threading.Event()

        # Build sub-module configs with global section included
        global_cfg = self.config.get("global", {})
        
        segmenter_cfg = {**self.config.get("segmenter", {}), "global": global_cfg}
        analyzer_cfg = {**self.config.get("analyzer", {}), "global": global_cfg}
        projector_cfg = {**self.config.get("projector", {}), "global": global_cfg}

        # Initialize processing modules
        self.loader = RadarDataLoader(self.config)
        self.segmenter = RadarCellSegmenter(segmenter_cfg)
        self.analyzer = RadarCellAnalyzer(analyzer_cfg)
        self.projector = RadarCellProjector(projector_cfg)

        # Keep last 2 datasets so we can compute flow and projections
        self.dataset_history = []  # List of (filepath, ds) tuples
        self.max_history = 2

        # SQLite database connection
        self.db_path = self._get_db_path()
        self.db_conn = None
        self.output_lock = threading.Lock()
        self._init_database()

    def _get_db_path(self) -> Path:
        """Get SQLite database path at radar-level (persists across runs/dates).

        Database is stored at: analysis/{radar_id}_cells_statistics.db
        This ensures all cells from a pipeline run (even across multiple dates)
        go into the same database.
        """
        output_dirs = self.config.get("output_dirs", {})
        radar_id = self.config.get("downloader", {}).get("radar_id", "UNKNOWN")

        analysis_dir = Path(output_dirs.get("analysis", "."))
        analysis_dir.mkdir(parents=True, exist_ok=True)
        db_filename = f"{radar_id}_cells_statistics.db"
        return analysis_dir / db_filename

    def _init_database(self):
        """Initialize SQLite database (schema created on first insert)."""
        self.db_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.db_initialized = False
        logger.info(f"Database initialized: {self.db_path}")

    def _create_cells_table(self, df_cells: pd.DataFrame):
        """Create cells table with explicit schema including all expected columns.
        
        Creates columns for all possible centroid types (geom, mass, maxdbz, registration, projections),
        heading statistics, and radar variables. Missing columns are filled with NULL.
        
        Primary Key: (time, cell_label) where time = time_volume_start
        Includes time_volume_start, time, and time_volume_end for temporal tracking.
        
        This ensures consistent schema across all files regardless of which features are available.
        """
        # Define all expected columns with their types
        # First scan may not have projections (needs 2+ frames), but schema includes them for consistency
        expected_columns = {
            # Temporal keys - all TIMESTAMP
            "time_volume_start": "TIMESTAMP NOT NULL",  # Start of radar volume scan
            "time": "TIMESTAMP NOT NULL",  # Analysis time (= time_volume_start)
            "time_volume_end": "TIMESTAMP",  # End of radar volume scan (future: when available)
            # Cell identifier
            "cell_label": "INTEGER NOT NULL",
            # Basic properties
            "cell_area_sqkm": "REAL",
            # Geometric centroid (center of mass)
            "cell_centroid_geom_x": "INTEGER",
            "cell_centroid_geom_y": "INTEGER",
            "cell_centroid_geom_lat": "REAL",
            "cell_centroid_geom_lon": "REAL",
            # Mass-weighted centroid (reflectivity weighted)
            "cell_centroid_mass_x": "INTEGER",
            "cell_centroid_mass_y": "INTEGER",
            "cell_centroid_mass_lat": "REAL",
            "cell_centroid_mass_lon": "REAL",
            # Max reflectivity centroid
            "cell_centroid_maxdbz_x": "INTEGER",
            "cell_centroid_maxdbz_y": "INTEGER",
            "cell_centroid_maxdbz_lat": "REAL",
            "cell_centroid_maxdbz_lon": "REAL",
            # Registration centroid (projection index 0 - frame-to-frame tracking)
            "cell_centroid_registration_x": "INTEGER",
            "cell_centroid_registration_y": "INTEGER",
            "cell_centroid_registration_lat": "REAL",
            "cell_centroid_registration_lon": "REAL",
            # Forward projection centroids (indices 1-5, configurable)
            "cell_centroid_projection1_x": "INTEGER",
            "cell_centroid_projection1_y": "INTEGER",
            "cell_centroid_projection1_lat": "REAL",
            "cell_centroid_projection1_lon": "REAL",
            "cell_centroid_projection2_x": "INTEGER",
            "cell_centroid_projection2_y": "INTEGER",
            "cell_centroid_projection2_lat": "REAL",
            "cell_centroid_projection2_lon": "REAL",
            "cell_centroid_projection3_x": "INTEGER",
            "cell_centroid_projection3_y": "INTEGER",
            "cell_centroid_projection3_lat": "REAL",
            "cell_centroid_projection3_lon": "REAL",
            "cell_centroid_projection4_x": "INTEGER",
            "cell_centroid_projection4_y": "INTEGER",
            "cell_centroid_projection4_lat": "REAL",
            "cell_centroid_projection4_lon": "REAL",
            "cell_centroid_projection5_x": "INTEGER",
            "cell_centroid_projection5_y": "INTEGER",
            "cell_centroid_projection5_lat": "REAL",
            "cell_centroid_projection5_lon": "REAL",
            # Motion vector statistics
            "cell_heading_x_mean": "REAL",
            "cell_heading_y_mean": "REAL",
            # Projection centroids as JSON (compact storage)
            "cell_projection_centroids_json": "TEXT",
        }
        
        # Add radar variable statistics (from config whitelist)
        radar_vars = self.config.get("analyzer", {}).get("radar_variables", [
            "reflectivity", "velocity", "differential_phase",
            "differential_reflectivity", "cross_correlation_ratio", "spectrum_width"
        ])
        for var in radar_vars:
            for stat in ["mean", "min", "max"]:
                col_name = f"radar_{var}_{stat}"
                expected_columns[col_name] = "REAL"
        
        # Merge with any extra columns from current DataFrame
        # (in case analyzer adds new columns we haven't anticipated)
        col_defs = []
        
        # Add columns in a logical order: keys, geometry, centroids, motion, radar
        column_order = ["time_volume_start", "time", "time_volume_end", "cell_label", "cell_area_sqkm"]
        
        # Add all expected columns
        for col_name in sorted(expected_columns.keys()):
            if col_name not in column_order:
                column_order.append(col_name)
        
        # Add any extra columns from current df that we didn't anticipate
        for col in df_cells.columns:
            if col not in expected_columns:
                column_order.append(col)
        
        # Build column definitions
        for col in column_order:
            if col in expected_columns:
                col_defs.append(f'"{col}" {expected_columns[col]}')
            else:
                # Dynamically determine type for unexpected columns
                if col.endswith("_x") or col.endswith("_y"):
                    col_defs.append(f'"{col}" INTEGER')
                elif col.endswith("_lat") or col.endswith("_lon"):
                    col_defs.append(f'"{col}" REAL')
                elif col.startswith("radar_"):
                    col_defs.append(f'"{col}" REAL')
                elif "_json" in col:
                    col_defs.append(f'"{col}" TEXT')
                else:
                    col_defs.append(f'"{col}" TEXT')
        
        col_defs_str = ",\n    ".join(col_defs)
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS cells (
            {col_defs_str},
            PRIMARY KEY (time, cell_label)
        )
        """
        self.db_conn.execute(create_sql)
        self.db_conn.execute("CREATE INDEX IF NOT EXISTS idx_time ON cells(time)")
        self.db_conn.execute("CREATE INDEX IF NOT EXISTS idx_time_volume_start ON cells(time_volume_start)")
        self.db_conn.commit()
        logger.info("ðŸ“Š Database schema created with composite PK (time, cell_label)")
        logger.info(f"   Temporal columns: time_volume_start, time, time_volume_end (TIMESTAMP)")
        logger.info(f"   Total columns: {len(col_defs)} (includes future-proof centroid/heading/projection slots)")

    def stop(self):
        """Signal processor to stop gracefully."""
        self._stop_event.set()

    def close_database(self):
        """Close database connection (call after final save)."""
        if self.db_conn:
            self.db_conn.close()
            logger.info("Database connection closed")

    def stopped(self):
        """Check if processor should stop."""
        return self._stop_event.is_set()


    def process_file(self, filepath: str) -> bool:
        """Process single file: load â†’ regrid â†’ segment â†’ project â†’ analyze â†’ save."""
        try:
            # Step 0: Handle dict input and check tracker for completed/plotting
            if isinstance(filepath, dict):
                filepath = filepath["path"]
            
            # Validate file exists before attempting to process
            file_path = Path(filepath)
            if not file_path.exists():
                logger.error("File not found: %s", filepath)
                return False
            
            file_id = Path(filepath).stem
            tracker = self.config.get("file_tracker")
            if tracker and tracker.should_process(file_id, "analyzed") is False:
                if tracker.should_process(file_id, "plotted") is False:
                    logger.info("â­ï¸  Skipping already completed: %s", Path(filepath).name)
                    return True
                else:
                    self._requeue_for_plotting(file_id, filepath)
                    return True

            logger.info("Processing: %s", Path(filepath).name)

            # Step 1: Load and regrid
            ds, ds_2d, nc_full_path, scan_time = self._load_and_regrid(filepath)
            if ds is None or ds_2d is None:
                logger.warning("Failed to load/regrid: %s", filepath)
                return False

            # Step 2: Segment
            ds_2d = self.segmenter.segment(ds_2d)
            var_names = self.config.get("global", {}).get("var_names", {})
            labels_name = var_names.get("cell_labels", "cell_labels")
            if labels_name not in ds_2d.data_vars:
                logger.warning("Segmentation failed for: %s", filepath)
                return False
            num_cells = int(ds_2d[labels_name].max().item())
            logger.info("Segmented: %d cells", num_cells)

            # Step 3: PROJECT (before analyzer so it has projection info)
            ds_2d = self._compute_projections(ds_2d, filepath)

            # Step 4: Save segmentation NetCDF for visualization (no dependency on analysis)
            seg_nc_path = self._save_segmentation_netcdf(ds_2d, filepath, scan_time)
            if seg_nc_path and self.output_queue:
                try:
                    item = {
                        'segmentation_nc': seg_nc_path,
                        'gridnc_file': None,
                        'radar_id': self.config.get("downloader", {}).get("radar_id", "UNKNOWN"),
                        'timestamp': scan_time,
                    }
                    self.output_queue.put_nowait(item)
                    logger.debug(f"Pushed to plotter queue: {seg_nc_path}")
                except queue.Full:
                    logger.debug("Plotter queue full, skipping frame")

            # Step 5: Analyze (parallel with visualization - no dependency)
            result = self._analyze_and_save(ds_2d, filepath, nc_full_path, num_cells, scan_time)
            
            return result


        except Exception as e:
            logger.exception("Error processing %s", filepath)
            tracker = self.config.get("file_tracker")
            if tracker:
                file_id = Path(filepath).stem
                tracker.mark_stage_complete(file_id, "analyzed", error=str(e))
            return False

    def _requeue_for_plotting(self, file_id, filepath):
        logger.info("Already analyzed, re-queuing for plot: %s", Path(filepath).name)
        output_dirs = self.config.get("output_dirs", {})
        radar_id = self.config.get("downloader", {}).get("radar_id", "UNKNOWN")
        if output_dirs and self.output_queue:
            from adapt.setup_directories import get_analysis_path
            from datetime import datetime, timezone
            try:
                parts = file_id.split('_')
                datetime_str = parts[0][-8:] + parts[1]
                scan_time = datetime.strptime(datetime_str, '%Y%m%d%H%M%S')
            except:
                scan_time = datetime.now(timezone.utc)
            seg_nc_path = get_analysis_path(
                output_dirs=output_dirs,
                radar_id=radar_id,
                analysis_type="segmentation",
                timestamp=scan_time,
                filename=f"{file_id}_analysis.nc"
            )
            if seg_nc_path.exists():
                try:
                    item = {
                        'segmentation_nc': seg_nc_path,
                        'gridnc_file': None,
                        'radar_id': radar_id,
                        'timestamp': scan_time,
                    }
                    self.output_queue.put_nowait(item)
                    logger.debug("Re-queued for plotting: %s", seg_nc_path.name)
                except queue.Full:
                    pass

    def _load_and_regrid(self, filepath):
        """Load and regrid radar file, return ds, ds_2d, nc_full_path, scan_time."""
        grid_kwargs = get_grid_kwargs()
        save_netcdf = self.config.get("regridder", {}).get("save_netcdf", True)
        output_dirs = self.config.get("output_dirs")
        nc_full_path = None
        scan_time = None
        if output_dirs:
            from adapt.setup_directories import get_netcdf_path
            from datetime import datetime, timezone
            radar_id = self.config.get("downloader", {}).get("radar_id", "UNKNOWN")
            nc_filename = Path(filepath).stem
            try:
                parts = nc_filename.split('_')
                datetime_str = parts[0][-8:] + parts[1]
                scan_time = datetime.strptime(datetime_str, '%Y%m%d%H%M%S')
            except:
                scan_time = datetime.now(timezone.utc)
            nc_full_path = get_netcdf_path(output_dirs, radar_id, nc_filename, scan_time=scan_time)
            output_dir = str(nc_full_path.parent)
        else:
            output_dir = self.config.get("downloader", {}).get("output_dir")
        ds = None
        if nc_full_path and nc_full_path.exists() and nc_full_path.stat().st_size > 1000:
            try:
                import pyart
                grid = pyart.io.read_grid(str(nc_full_path))
                ds = grid.to_xarray()
                logger.debug("Loaded existing NetCDF: %s", nc_full_path.name)
            except Exception as e:
                logger.warning("Failed to load existing NetCDF %s: %s", nc_full_path.name, e)
                ds = None
        if ds is None:
            ds = self.loader.load_and_regrid(filepath, grid_kwargs=grid_kwargs,
                                            save_netcdf=save_netcdf, output_dir=output_dir)
        if ds is None:
            return None, None, nc_full_path, scan_time
        ds_2d = self._extract_2d_slice(ds)
        logger.debug(f"Extracted 2D slice at z-level, shape: {ds_2d.dims}")
        return ds, ds_2d, nc_full_path, scan_time

    def _compute_projections(self, ds_2d: xr.Dataset, filepath: str) -> xr.Dataset:
        """Compute optical flow projections if 2+ frames available.
        
        Parameters
        ----------
        ds_2d : xr.Dataset
            Segmented 2D dataset
        filepath : str
            Current file path (for history tracking)
            
        Returns
        -------
        xr.Dataset
            Dataset with projections added (flow_u, flow_v, cell_projections),
            or original ds_2d if insufficient frames
        """
        # Update history with current dataset
        self.dataset_history.append((filepath, ds_2d))
        if len(self.dataset_history) > self.max_history:
            self.dataset_history.pop(0)
        
        # Need 2+ frames for projection
        if len(self.dataset_history) < 2:
            logger.debug("First frame: optical flow not available (need 2+ datasets)")
            return ds_2d
        
        try:
            ds_prev_path, ds_prev = self.dataset_history[-2]
            ds_curr_path, ds_curr = self.dataset_history[-1]
            
            # Projector returns 2D ds with cell_projections, flow_u, flow_v added
            ds_with_proj = self.projector.project([ds_prev, ds_curr])
            if ds_with_proj is not None:
                logger.info(f"âœ“ Projections computed: {list(ds_with_proj.data_vars)}")
                return ds_with_proj
        except Exception as e:
            logger.error(f"Optical flow projection failed: {e}", exc_info=True)
        
        return ds_2d

    def _get_table_columns(self) -> list:
        """Get list of column names from existing cells table."""
        try:
            cursor = self.db_conn.execute("PRAGMA table_info(cells)")
            return [row[1] for row in cursor.fetchall()]
        except:
            return []

    def _align_dataframe_to_schema(self, df_cells: pd.DataFrame) -> pd.DataFrame:
        """Align DataFrame columns to existing table schema.
        
        Ensures all columns from the table schema are present in the DataFrame,
        filling missing columns with NULL values. This handles cases where:
        - First file creates schema with only available columns
        - Subsequent files have new columns from enhanced extraction
        - Early frames lack projection data (need 2+ frames)
        """
        existing_cols = self._get_table_columns()
        if existing_cols:
            # Add missing columns with NULL values
            missing_cols = [c for c in existing_cols if c not in df_cells.columns]
            if missing_cols:
                logger.debug("DataFrame missing %d columns: %s (filling with NULL)", len(missing_cols), missing_cols[:5])
                for col in missing_cols:
                    df_cells[col] = None
            
            # Ensure column order matches table schema
            df_cells = df_cells[existing_cols]
        return df_cells

    def _analyze_and_save(self, ds_2d, filepath, nc_full_path, num_cells, scan_time):
        """Analyze cells (now has projection info), update tracker, insert into SQLite DB."""
        z_level = self.config.get("global", {}).get("z_level", 2000)
        df_cells = self.analyzer.extract(ds_2d, z_level=z_level)
        tracker = self.config.get("file_tracker")
        file_id = Path(filepath).stem
        if tracker and nc_full_path:
            tracker.mark_stage_complete(file_id, "regridded", path=nc_full_path)
        if df_cells.empty:
            logger.debug("No cells detected in: %s", Path(filepath).name)
            if tracker:
                tracker.mark_stage_complete(file_id, "analyzed", num_cells=0)
            return True

        # Ensure time columns are datetime for SQLite TIMESTAMP
        time_cols = ["time_volume_start", "time", "time_volume_end"]
        for col in time_cols:
            if col in df_cells.columns:
                df_cells[col] = pd.to_datetime(df_cells[col], errors='coerce')

        logger.debug("Analyzed: %d cells with projection info", len(df_cells))
        self._log_cell_statistics(df_cells)
        
        with self.output_lock:
            if not self.db_initialized:
                self._create_cells_table(df_cells)
                self.db_initialized = True
            else:
                # Align DataFrame to existing table schema (handles files with different variables)
                df_cells = self._align_dataframe_to_schema(df_cells)
            # Append data (schema already exists)
            df_cells.to_sql('cells', self.db_conn, if_exists='append', index=False)
        
        logger.info("âœ“ Successfully processed: %s (saved %d cells to DB)", Path(filepath).name, len(df_cells))
        if tracker:
            tracker.mark_stage_complete(file_id, "analyzed", num_cells=len(df_cells))
        return True

    def run(self):
        """Main processor loop (runs in thread).

        Continuously reads filepaths from input_queue, processes each file through
        the complete scientific pipeline, and sends results to output_queue for
        visualization. Exits when None (sentinel) is received.

        Notes
        -----
        Called automatically by thread.start(). Do not call directly.
        """
        logger.info("Processor started, waiting for files...")

        while not self.stopped():
            try:
                # Get next file from queue (block for 1 second)
                try:
                    filepath = self.input_queue.get(timeout=1)
                except queue.Empty:
                    continue

                # Process file (always mark as done even if it fails)
                try:
                    success = self.process_file(filepath)
                except Exception as e:
                    logger.exception("Failed to process file: %s", filepath)
                finally:
                    # Always mark task as done to prevent queue from blocking
                    self.input_queue.task_done()

            except Exception as e:
                logger.exception("Processor error")

        logger.info("Processor stopped")


    def get_results(self) -> pd.DataFrame:
        """Return all processed cell statistics as a DataFrame.

        Thread-safe query of the SQLite database containing all cells extracted
        from processed files. Can be called while processor is running (WAL mode
        enables concurrent access).

        Returns
        -------
        pd.DataFrame
            One row per detected cell with columns for time, geometry, reflectivity,
            motion, and projections. Empty DataFrame if no processing completed yet.
        """
        with self.output_lock:
            # Don't query before schema is created
            if not self.db_initialized:
                return pd.DataFrame()

            try:
                return pd.read_sql("SELECT * FROM cells", self.db_conn)
            except Exception as e:
                logger.warning("Failed to read from database: %s", e)
                return pd.DataFrame()

    def save_results(self, filepath: str = None):
        """Export all cell statistics to Parquet file.

        Creates a single Parquet file containing all cells from the SQLite
        database. Parquet format is efficient for subsequent analysis with Pandas,
        DuckDB, Polars, or other analytical tools.

        Parameters
        ----------
        filepath : str, optional
            Output Parquet filepath. If None, uses:
            `{output_dirs['analysis']}/{radar_id}_cells_statistics.parquet`
        """
        if filepath is None:
            filepath = get_output_path()

        try:
            filepath = Path(filepath)
            filepath.parent.mkdir(parents=True, exist_ok=True)

            with self.output_lock:
                cursor = self.db_conn.execute("SELECT COUNT(*) FROM cells")
                count = cursor.fetchone()[0]

                if count > 0:
                    df = pd.read_sql("SELECT * FROM cells", self.db_conn)
                    df.to_parquet(filepath, engine='pyarrow',
                                  compression=self.config["output"]["compression"],
                                  index=False)
                    logger.info("Exported %d rows to: %s", count, filepath)
                else:
                    logger.warning("No results to export")

        except Exception as e:
            logger.exception("Failed to export results")

    def _log_cell_statistics(self, df_cells: pd.DataFrame):
        """Log detailed cell statistics for each detected cell.

        Parameters
        ----------
        df_cells : pd.DataFrame
            DataFrame with cell properties from analyzer.
        """
        try:
            if df_cells.empty:
                return

            # Extract key statistics from DataFrame
            num_cells = len(df_cells)

            # Build statistics log line
            stats_parts = [f"Cells: {num_cells}"]

            # Area statistics
            if "area_km2" in df_cells.columns:
                area_vals = df_cells["area_km2"].dropna()
                if len(area_vals) > 0:
                    stats_parts.append(
                        f"Area [kmÂ²] - min={area_vals.min():.1f}, "
                        f"max={area_vals.max():.1f}, "
                        f"mean={area_vals.mean():.1f}"
                    )

            # Reflectivity statistics
            if "reflectivity_mean" in df_cells.columns:
                refl_mean_vals = df_cells["reflectivity_mean"].dropna()
                if len(refl_mean_vals) > 0:
                    stats_parts.append(
                        f"Refl-mean [dBZ] - min={refl_mean_vals.min():.1f}, "
                        f"max={refl_mean_vals.max():.1f}, "
                        f"mean={refl_mean_vals.mean():.1f}"
                    )

            if "reflectivity_max" in df_cells.columns:
                refl_max_vals = df_cells["reflectivity_max"].dropna()
                if len(refl_max_vals) > 0:
                    stats_parts.append(
                        f"Refl-max [dBZ] - min={refl_max_vals.min():.1f}, "
                        f"max={refl_max_vals.max():.1f}, "
                        f"mean={refl_max_vals.mean():.1f}"
                    )

            # Log summary
            summary_msg = " | ".join(stats_parts)
            logger.info("ðŸ“Š Cell Statistics: %s", summary_msg)

            # Log individual cell details for small number of cells
            if num_cells <= 5:
                for idx, row in df_cells.iterrows():
                    cell_label = row.get("cell_label", "?")
                    area = row.get("cell_area_sqkm", np.nan)
                    refl_mean = row.get("radar_reflectivity_mean", np.nan)
                    refl_max = row.get("radar_reflectivity_max", np.nan)

                    detail_msg = f"Cell #{cell_label}: area={area:.1f} kmÂ², refl_mean={refl_mean:.1f} dBZ, refl_max={refl_max:.1f} dBZ"
                    logger.info("  â””â”€ %s", detail_msg)

        except Exception as e:
            logger.debug("Could not log cell statistics: %s", e)

    def _extract_2d_slice(self, ds: xr.Dataset) -> xr.Dataset:
        """Extract 2D slice at configured z-level.
        
        Parameters
        ----------
        ds : xr.Dataset
            Full 3D dataset from loader
            
        Returns
        -------
        xr.Dataset
            2D dataset with all variables sliced at z-level
        """
        global_cfg = self.config.get("global", {})
        z_level = global_cfg.get("z_level", 2000)
        coord_names = global_cfg.get("coord_names", {})
        time_name = coord_names.get("time", "time")
        z_name = coord_names.get("z", "z")
        
        # Find z-level index
        z_idx = int(np.argmin(np.abs(ds[z_name].values - z_level)))
        
        # Extract 2D slice for all 3D variables
        ds_2d = xr.Dataset()
        for var_name in ds.data_vars:
            var = ds[var_name]
            # Check if variable has time and z dimensions
            if time_name in var.dims and z_name in var.dims:
                # Slice to 2D
                ds_2d[var_name] = var.isel({time_name: 0, z_name: z_idx})
            else:
                # Keep as-is (already 2D or scalar)
                ds_2d[var_name] = var
        
        # Copy coordinates (only y, x for 2D)
        ds_2d = ds_2d.assign_coords({
            "y": ds.y,
            "x": ds.x,
        })
        
        # Copy attributes
        ds_2d.attrs.update(ds.attrs)
        ds_2d.attrs["z_level_m"] = float(ds[z_name].values[z_idx])
        
        logger.debug(f"Extracted 2D slice at z={ds[z_name].values[z_idx]:.0f}m (index {z_idx})")
        return ds_2d

    def _save_segmentation_netcdf(self, ds: xr.Dataset, filepath: str, scan_time) -> Optional[str]:
        """Save analysis results to NetCDF for visualization.
        """
        try:
            output_dirs = self.config.get("output_dirs")
            if not output_dirs:
                return None

            from adapt.setup_directories import get_analysis_path
            radar_id = self.config.get("downloader", {}).get("radar_id", "UNKNOWN")

            # Create output path
            filename_stem = Path(filepath).stem
            seg_nc_filename = f"{filename_stem}_analysis.nc"
            seg_nc_path = get_analysis_path(
                output_dirs,
                radar_id=radar_id,
                analysis_type="netcdf",
                timestamp=scan_time,
                filename=seg_nc_filename
            )
            seg_nc_path.parent.mkdir(parents=True, exist_ok=True)

            # Add metadata
            ds.attrs.update({
                "source": str(filepath),
                "radar_id": radar_id,
                "description": "Radar analysis with segmentation and projections"
            })

            # Save to NetCDF
            ds.to_netcdf(seg_nc_path, mode='w', engine='netcdf4', format='NETCDF4', compute=True)
            ds.close()

            # Log what was saved
            components = list(ds.data_vars.keys())
            logger.info(f"âœ“ Analysis saved: {seg_nc_path.name} [{', '.join(components)}]")

            return str(seg_nc_path)

        except Exception as e:
            logger.warning(f"Could not save segmentation NetCDF: {e}")
            return None


if __name__ == "__main__":
    # Logging configured by pipeline orchestrator
    pass  # Placeholder for demo code below
    print("Processor classes loaded. Use in pipeline orchestration.")
