{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAPT Data API Demo\n",
    "\n",
    "This notebook demonstrates the **DataAPI** — the subscription-based read interface\n",
    "for downstream consumers of ADAPT pipeline outputs.\n",
    "\n",
    "## Architecture Recap\n",
    "\n",
    "```\n",
    "Thread 1 (Acquisition)  →  DataStore  ←  Thread 2 (Processing)\n",
    "                              ↑\n",
    "                     Thread 3 (External)\n",
    "                     This notebook / scripts\n",
    "```\n",
    "\n",
    "- The **DataStore** (SQLite) is the sole communication channel.\n",
    "- The **DataAPI** wraps the store with a read-only, subscription-based interface.\n",
    "- External scripts **never** import pipeline internals — they only use `DataAPI`.\n",
    "\n",
    "## Two Data Kinds\n",
    "\n",
    "| Kind   | Format  | Reader           | Examples                            |\n",
    "|--------|---------|------------------|-------------------------------------|\n",
    "| Grid   | NetCDF  | `xr.Dataset`     | Reflectivity, segmentation masks, flow vectors |\n",
    "| Series | SQLite  | `pd.DataFrame`   | Cell statistics timeseries          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We create a temporary DataStore, register some fake products, and then use the\n",
    "DataAPI to read them — exactly as a real visualization or analysis script would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from adapt.core.store import DataStore\n",
    "from adapt.data_access import DataAPI, Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary workspace\n",
    "tmpdir = Path(tempfile.mkdtemp(prefix=\"adapt_demo_\"))\n",
    "db_path = tmpdir / \"adapt.db\"\n",
    "print(f\"Working directory: {tmpdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating Pipeline Output\n",
    "\n",
    "In production, the pipeline writes NetCDF files and registers them in the DataStore.\n",
    "Here we simulate that by creating fake grid and series products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_gridded_netcdf(path, scan_time, shape=(100, 100)):\n",
    "    \"\"\"Create a fake gridded reflectivity NetCDF.\"\"\"\n",
    "    ny, nx = shape\n",
    "    y = np.linspace(-150, 150, ny)  # km\n",
    "    x = np.linspace(-150, 150, nx)  # km\n",
    "    \n",
    "    # Simulate reflectivity with a few storm cells\n",
    "    Y, X = np.meshgrid(y, x, indexing='ij')\n",
    "    refl = np.full(shape, -10.0, dtype='float32')\n",
    "    \n",
    "    # Cell 1: centered at (30, -50)\n",
    "    r1 = np.sqrt((Y - 30)**2 + (X + 50)**2)\n",
    "    refl += 55 * np.exp(-r1**2 / (2 * 20**2))\n",
    "    \n",
    "    # Cell 2: centered at (-20, 60)\n",
    "    r2 = np.sqrt((Y + 20)**2 + (X - 60)**2)\n",
    "    refl += 45 * np.exp(-r2**2 / (2 * 15**2))\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        {'reflectivity': (('y', 'x'), refl)},\n",
    "        coords={'y': y, 'x': x},\n",
    "        attrs={\n",
    "            'scan_time': scan_time.isoformat(),\n",
    "            'radar_id': 'KLOT',\n",
    "            'z_level_m': 2000,\n",
    "        },\n",
    "    )\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ds.to_netcdf(path)\n",
    "    ds.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def create_fake_segmented_netcdf(path, scan_time, shape=(100, 100)):\n",
    "    \"\"\"Create a fake segmented NetCDF with reflectivity, cell_labels, and heading vectors.\"\"\"\n",
    "    ny, nx = shape\n",
    "    y = np.linspace(-150, 150, ny)\n",
    "    x = np.linspace(-150, 150, nx)\n",
    "    Y, X = np.meshgrid(y, x, indexing='ij')\n",
    "    \n",
    "    # Reflectivity\n",
    "    refl = np.full(shape, -10.0, dtype='float32')\n",
    "    r1 = np.sqrt((Y - 30)**2 + (X + 50)**2)\n",
    "    refl += 55 * np.exp(-r1**2 / (2 * 20**2))\n",
    "    r2 = np.sqrt((Y + 20)**2 + (X - 60)**2)\n",
    "    refl += 45 * np.exp(-r2**2 / (2 * 15**2))\n",
    "    \n",
    "    # Cell labels (0 = background, 1 = cell 1, 2 = cell 2)\n",
    "    labels = np.zeros(shape, dtype='int32')\n",
    "    labels[r1 < 30] = 1\n",
    "    labels[r2 < 25] = 2\n",
    "    \n",
    "    # Heading vectors (simulated storm motion)\n",
    "    heading_x = np.where(labels > 0, 5.0, 0.0).astype('float32')\n",
    "    heading_y = np.where(labels > 0, 2.0, 0.0).astype('float32')\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            'reflectivity': (('y', 'x'), refl),\n",
    "            'cell_labels': (('y', 'x'), labels),\n",
    "            'heading_x': (('y', 'x'), heading_x),\n",
    "            'heading_y': (('y', 'x'), heading_y),\n",
    "        },\n",
    "        coords={'y': y, 'x': x},\n",
    "        attrs={\n",
    "            'scan_time': scan_time.isoformat(),\n",
    "            'radar_id': 'KLOT',\n",
    "            'z_level_m': 2000,\n",
    "        },\n",
    "    )\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ds.to_netcdf(path)\n",
    "    ds.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def create_fake_analysis_sqlite(path, scan_times):\n",
    "    \"\"\"Create a fake analysis SQLite with cell statistics.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(str(path))\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE cells (\n",
    "            scan_time TEXT,\n",
    "            cell_label INTEGER,\n",
    "            reflectivity_max REAL,\n",
    "            reflectivity_mean REAL,\n",
    "            area_km2 REAL,\n",
    "            centroid_x REAL,\n",
    "            centroid_y REAL\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    rows = []\n",
    "    for t in scan_times:\n",
    "        for cell_id in [1, 2]:\n",
    "            rows.append((\n",
    "                t.isoformat(),\n",
    "                cell_id,\n",
    "                round(40 + np.random.rand() * 20, 1),\n",
    "                round(25 + np.random.rand() * 15, 1),\n",
    "                round(50 + np.random.rand() * 100, 1),\n",
    "                round(-50 + cell_id * 80 + np.random.randn() * 5, 1),\n",
    "                round(30 - cell_id * 40 + np.random.randn() * 5, 1),\n",
    "            ))\n",
    "    \n",
    "    conn.executemany(\n",
    "        \"INSERT INTO cells VALUES (?, ?, ?, ?, ?, ?, ?)\",\n",
    "        rows,\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a sequence of 5 scans (5 minutes apart)\n",
    "base_time = datetime(2025, 6, 15, 18, 0, 0)\n",
    "scan_times = [base_time + timedelta(minutes=5 * i) for i in range(5)]\n",
    "\n",
    "store = DataStore(db_path)\n",
    "\n",
    "# Register gridded + segmented NetCDFs\n",
    "for t in scan_times:\n",
    "    ts = t.strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Gridded\n",
    "    grid_path = create_fake_gridded_netcdf(\n",
    "        tmpdir / f\"gridded/KLOT_{ts}_gridded.nc\", t\n",
    "    )\n",
    "    grid_id = store.register_product(\n",
    "        product_type=\"gridded_netcdf\",\n",
    "        file_path=grid_path,\n",
    "        radar_id=\"KLOT\",\n",
    "        scan_time=t,\n",
    "        producer_module=\"acquisition\",\n",
    "    )\n",
    "    \n",
    "    # Segmented\n",
    "    seg_path = create_fake_segmented_netcdf(\n",
    "        tmpdir / f\"segmented/KLOT_{ts}_segmented.nc\", t\n",
    "    )\n",
    "    store.register_product(\n",
    "        product_type=\"segmented_netcdf\",\n",
    "        file_path=seg_path,\n",
    "        radar_id=\"KLOT\",\n",
    "        scan_time=t,\n",
    "        producer_module=\"detection\",\n",
    "        parent_ids=[grid_id],\n",
    "    )\n",
    "\n",
    "# Register analysis SQLite\n",
    "analysis_path = create_fake_analysis_sqlite(\n",
    "    tmpdir / \"analysis/KLOT_cells.db\", scan_times\n",
    ")\n",
    "store.register_product(\n",
    "    product_type=\"analysis_sqlite\",\n",
    "    file_path=analysis_path,\n",
    "    radar_id=\"KLOT\",\n",
    "    scan_time=scan_times[-1],\n",
    "    producer_module=\"analysis\",\n",
    ")\n",
    "\n",
    "stats = store.get_statistics(radar_id=\"KLOT\")\n",
    "print(f\"Registered {stats['total_products']} products:\")\n",
    "for ptype, count in stats['products_by_type'].items():\n",
    "    print(f\"  {ptype}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the DataAPI\n",
    "\n",
    "From this point, we use **only** the `DataAPI`. This is the same interface a real\n",
    "visualization script or Jupyter analysis session would use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DataAPI.from_path(db_path)\n",
    "print(f\"DataAPI connected to: {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. List Available Grid Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all segmented grids\n",
    "seg_products = api.list_grids(\n",
    "    product_type=\"segmented_netcdf\",\n",
    "    radar_id=\"KLOT\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(seg_products)} segmented grids:\")\n",
    "for p in seg_products:\n",
    "    print(f\"  {p.product_id[:8]}...  scan={p.scan_time}  file={Path(p.file_path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Read a Grid Product (NetCDF → xr.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the most recent segmented grid\n",
    "ds = api.get_latest_grid(\"segmented_netcdf\", \"KLOT\")\n",
    "\n",
    "print(f\"Scan time: {ds.attrs['scan_time']}\")\n",
    "print(f\"Variables: {list(ds.data_vars)}\")\n",
    "print(f\"Grid shape: {ds['reflectivity'].shape}\")\n",
    "print(f\"Cells detected: {len(np.unique(ds['cell_labels'].values)) - 1}\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Visualize Grid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Reflectivity\n",
    "ds['reflectivity'].plot(ax=axes[0], cmap='turbo', vmin=-10, vmax=70)\n",
    "axes[0].set_title('Reflectivity (dBZ)')\n",
    "\n",
    "# Cell labels\n",
    "labels = ds['cell_labels'].values\n",
    "masked = np.ma.masked_where(labels == 0, labels)\n",
    "axes[1].imshow(masked, origin='lower', cmap='tab20', alpha=0.8,\n",
    "               extent=[ds.x.min(), ds.x.max(), ds.y.min(), ds.y.max()])\n",
    "axes[1].set_title(f'Cell Labels ({len(np.unique(labels)) - 1} cells)')\n",
    "axes[1].set_xlabel('x (km)')\n",
    "axes[1].set_ylabel('y (km)')\n",
    "\n",
    "# Heading vectors overlaid on reflectivity\n",
    "ds['reflectivity'].plot(ax=axes[2], cmap='gray', alpha=0.4, add_colorbar=False)\n",
    "step = 10\n",
    "Y, X = np.meshgrid(ds.y.values[::step], ds.x.values[::step], indexing='ij')\n",
    "hx = ds['heading_x'].values[::step, ::step]\n",
    "hy = ds['heading_y'].values[::step, ::step]\n",
    "mask = ds['cell_labels'].values[::step, ::step] > 0\n",
    "axes[2].quiver(X[mask], Y[mask], hx[mask], hy[mask], color='red', scale=50)\n",
    "axes[2].set_title('Heading Vectors')\n",
    "\n",
    "fig.suptitle(f\"KLOT — {ds.attrs['scan_time']}\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Read Series Data (SQLite → pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List series products\n",
    "series_products = api.list_series(\n",
    "    product_type=\"analysis_sqlite\",\n",
    "    radar_id=\"KLOT\",\n",
    ")\n",
    "print(f\"Found {len(series_products)} analysis product(s)\")\n",
    "\n",
    "# Read cell statistics\n",
    "df = api.read_series(\n",
    "    series_products[0],\n",
    "    table=\"cells\",\n",
    "    columns=[\"scan_time\", \"cell_label\", \"reflectivity_max\", \"area_km2\"],\n",
    ")\n",
    "print(f\"\\nCell statistics: {len(df)} rows\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with filters: only cell 1\n",
    "df_cell1 = api.read_series(\n",
    "    series_products[0],\n",
    "    table=\"cells\",\n",
    "    where={\"cell_label\": \"1\"},\n",
    ")\n",
    "print(f\"Cell 1 records: {len(df_cell1)}\")\n",
    "df_cell1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with time range filter\n",
    "df_recent = api.read_series(\n",
    "    series_products[0],\n",
    "    table=\"cells\",\n",
    "    after=\"2025-06-15T18:10:00\",\n",
    "    before=\"2025-06-15T18:20:00\",\n",
    ")\n",
    "print(f\"Records in time window: {len(df_recent)}\")\n",
    "df_recent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Visualize Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full timeseries for both cells\n",
    "df_all = api.read_series(\n",
    "    series_products[0],\n",
    "    table=\"cells\",\n",
    ")\n",
    "df_all['scan_time'] = pd.to_datetime(df_all['scan_time'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for cell_id, grp in df_all.groupby('cell_label'):\n",
    "    axes[0].plot(grp['scan_time'], grp['reflectivity_max'], 'o-', label=f'Cell {cell_id}')\n",
    "    axes[1].plot(grp['scan_time'], grp['area_km2'], 's-', label=f'Cell {cell_id}')\n",
    "\n",
    "axes[0].set_ylabel('Max Reflectivity (dBZ)')\n",
    "axes[0].set_title('Peak Reflectivity Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "\n",
    "axes[1].set_ylabel('Area (km²)')\n",
    "axes[1].set_title('Cell Area Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=30)\n",
    "\n",
    "fig.suptitle('Cell Statistics Timeseries — KLOT', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subscription Model\n",
    "\n",
    "The `Subscription` class enables poll-based notification: a downstream consumer\n",
    "subscribes to product types and polls for new data. Each `poll()` returns only\n",
    "products added since the last poll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subscription\n",
    "sub = api.subscribe(\n",
    "    product_types=[\"segmented_netcdf\"],\n",
    "    radar_id=\"KLOT\",\n",
    ")\n",
    "\n",
    "# First poll: gets all existing products\n",
    "batch1 = sub.poll()\n",
    "print(f\"First poll: {len(batch1)} products\")\n",
    "for p in batch1:\n",
    "    print(f\"  {p.scan_time} — {Path(p.file_path).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second poll: nothing new\n",
    "batch2 = sub.poll()\n",
    "print(f\"Second poll: {len(batch2)} products (nothing new)\")\n",
    "\n",
    "# has_new() check\n",
    "print(f\"Has new: {sub.has_new()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate pipeline producing a new scan\n",
    "new_time = base_time + timedelta(minutes=25)\n",
    "new_ts = new_time.strftime('%Y%m%d_%H%M%S')\n",
    "new_path = create_fake_segmented_netcdf(\n",
    "    tmpdir / f\"segmented/KLOT_{new_ts}_segmented.nc\", new_time\n",
    ")\n",
    "store.register_product(\n",
    "    product_type=\"segmented_netcdf\",\n",
    "    file_path=new_path,\n",
    "    radar_id=\"KLOT\",\n",
    "    scan_time=new_time,\n",
    "    producer_module=\"detection\",\n",
    ")\n",
    "\n",
    "# Now poll picks up only the new one\n",
    "print(f\"Has new: {sub.has_new()}\")\n",
    "batch3 = sub.poll()\n",
    "print(f\"Third poll: {len(batch3)} new product(s)\")\n",
    "for p in batch3:\n",
    "    print(f\"  {p.scan_time} — {Path(p.file_path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. get_latest() — Peek Without Advancing the Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = sub.get_latest()\n",
    "print(f\"Latest product: {latest.scan_time} — {Path(latest.file_path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Product Subscription\n",
    "\n",
    "Subscribe to multiple product types simultaneously. Useful for a script that\n",
    "needs both segmentation masks and analysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sub = api.subscribe(\n",
    "    product_types=[\"segmented_netcdf\", \"analysis_sqlite\"],\n",
    "    radar_id=\"KLOT\",\n",
    ")\n",
    "\n",
    "products = multi_sub.poll()\n",
    "print(f\"Multi-subscription poll: {len(products)} products\")\n",
    "for p in products:\n",
    "    print(f\"  [{p.product_type}] {p.scan_time} — {Path(p.file_path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Usage Pattern\n",
    "\n",
    "A typical visualization script would look like this:\n",
    "\n",
    "```python\n",
    "from adapt.data_access import DataAPI\n",
    "import time\n",
    "\n",
    "api = DataAPI.from_path(\"output/adapt.db\")\n",
    "\n",
    "sub = api.subscribe(\n",
    "    product_types=[\"segmented_netcdf\"],\n",
    "    radar_id=\"KLOT\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    for product in sub.poll():\n",
    "        ds = api.read_grid(product)\n",
    "        plot(ds)  # your plotting function\n",
    "    time.sleep(10)\n",
    "```\n",
    "\n",
    "A timeseries analysis script:\n",
    "\n",
    "```python\n",
    "api = DataAPI.from_path(\"output/adapt.db\")\n",
    "\n",
    "products = api.list_series(\n",
    "    product_type=\"analysis_sqlite\",\n",
    "    radar_id=\"KLOT\",\n",
    ")\n",
    "\n",
    "df = api.read_series(\n",
    "    products[0],\n",
    "    table=\"cells\",\n",
    "    columns=[\"scan_time\", \"cell_label\", \"reflectivity_max\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(tmpdir, ignore_errors=True)\n",
    "print(f\"Cleaned up {tmpdir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
